import io
import openai
from dotenv import load_dotenv
from typing import TypedDict, Annotated, Literal
from pydantic import BaseModel, Field
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END     # ê·¸ë˜í”„ ë° ì¢…ë£Œ ì§€ì 
from langgraph.graph.message import add_messages
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import ChatMessage #, BaseMessage, SystemMessage, HumanMessage, AIMessage
# from langchain_core.output_parsers import StrOutputParser

load_dotenv()

client = openai.OpenAI()
llm = ChatOpenAI(
    model_name='gpt-4o-mini-2024-07-18', 
    temperature=0.5, 
    streaming=True,
)


basic_styles = 'ìŠ¤íƒ€ì¼ì€ 10ê°€ì§€ë¡œ ë¬¸ì–´ì²´, êµ¬ì–´ì²´, ê°ì„±ì²´, ë§ˆì¼€íŒ…ì²´, ë‰´ìŠ¤ì²´, ë…¼ë¬¸ì²´, SNSì²´, ì‹œì ì²´, ì–´ë¦°ì´ì²´, ì‚¬ê·¹ì²´ ì…ë‹ˆë‹¤.'

system_prompt = """ë‹¹ì‹ ì€ í•œêµ­ì–´ ì „ë¬¸ê°€ ì…ë‹ˆë‹¤. 
í•œê¸€ë¡œ ì…ë ¥ëœ ë¬¸ì¥ì„ ë‹¤ìŒê³¼ ê°™ì€ ìŠ¤íƒ€ì¼ì— ë§ê²Œ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ í‘œí˜„í•´ ì£¼ì„¸ìš”."""

check_sentence_prompt = """
ë‹¤ìŒ ë¬¸ì¥ì´ ë¶ˆì™„ì „í•˜ê±°ë‚˜ ì˜ë¯¸ì—†ëŠ” ë¬´ì‘ìœ„ í‚¤ ì…ë ¥(ì˜ˆ: 'ã…ã„´ã…‡ã„¹ã…‡ã…ã„´ã…‡ã„¹')ì´ë¼ë©´ 'no'ë¥¼ ì¶œë ¥í•˜ì„¸ìš”.
ê·¸ ë¬¸ì¥ì´ í•œê¸€ ë¬¸ë²•, ë§ì¶¤ë²•, ë„ì–´ì“°ê¸°ë¡œ êµì • ê°€ëŠ¥í•˜ë©´ 'yes'ë¥¼ ì¶œë ¥í•˜ì„¸ìš”.

ë¬¸ì¥: {sentence}
"""

retify_prompt = """
ë‹¤ìŒ ë¬¸ì¥ì—ì„œ í•œê¸€ ë¬¸ë²•, ë§ì¶¤ë²•, ë„ì–´ì“°ê¸°ë¥¼ êµì •í•´ ì£¼ì„¸ìš”. ì˜ë¯¸ ë˜ëŠ” í‘œí˜„ ë³€ê²½ ì—†ì´ ì˜¤ì§ ë¬¸ë²•ë§Œ ìˆ˜ì •í•´ ì£¼ì„¸ìš”.

ë¬¸ì¥: {sentence}
"""
generate_prompt = """
ë‹¤ìŒ ì…ë ¥ê°’ì— ë”°ë¼ ë¬¸ì¥ì„ ìƒì„±í•˜ì„¸ìš”. 

[ì›ë¬¸] {sentence}
[ìŠ¤íƒ€ì¼] {styles}
[ë¬¸ì¥ê°œìˆ˜] {number}

[ì¶œë ¥í˜•ì‹]
- ì›ë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ ìŠ¤íƒ€ì¼ì— ë§ê²Œ ë¬¸ì¥ì„ ìƒì„±í•˜ì„¸ìš”.
- ê° ìŠ¤íƒ€ì¼ë§ˆë‹¤ {number}ê°œì˜ ë¬¸ì¥ë§Œ ìƒì„±í•˜ì„¸ìš”.
- ê° ë¬¸ì¥ì€ ì½”ë“œë¸”ë¡ìœ¼ë¡œ ê°ì‹¸ í´ë¦½ë³´ë“œ ë³µì‚¬ê°€ ê°€ëŠ¥í•´ì•¼ í•©ë‹ˆë‹¤.
- ì¶œë ¥ ì˜ˆì‹œëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

ì˜ˆì‹œ ì…ë ¥: 
[ì›ë¬¸] ì˜¤ëŠ˜ì€ ë‚ ì”¨ê°€ ì°¸ ì¢‹ë‹¤ 
[ìŠ¤íƒ€ì¼] ìŠ¤íƒ€ì¼ì€ SNSì²´ ì…ë‹ˆë‹¤ 
[ë¬¸ì¥ê°œìˆ˜] 2 

ì˜ˆì‹œ ì¶œë ¥:
**ì›ë¬¸:**
```
ì˜¤ëŠ˜ì€ ë‚ ì”¨ê°€ ì°¸ ì¢‹ë‹¤.  
```
**SNSì²´:**
```
ë‚ ì”¨ ë¯¸ì³¤ë‹¤ â˜€ï¸ #ë§‘ìŒ #í–‰ë³µ    
```
```
í™”ì°½í•œ í•œëŠ˜ê³¼ ìƒˆí•˜ì–€ ë­‰ê²Œêµ¬ë¦„ì´ ì™¸ì¶œì„ ë¶€ë¥´ëŠ”êµ¬ë‚˜ ğŸ’¨ #ì•¼ì™¸      
```

"""

def speech_to_text(voice):
    with io.BytesIO(voice.getvalue()) as file:
        file.name = 'voice.wav'
        transcription = client.audio.transcriptions.create(		# ìŒì„± ì¸ì‹
            model='whisper-1',		# ìŒì„± ì¸ì‹ ëª¨ë¸
            file=file,
            language='ko'
        )
    return transcription.text		# ìŒì„± ì¸ì‹ ê²°ê³¼ í…ìŠ¤íŠ¸


# ìƒíƒœ ëª¨ë¸(State) ì •ì˜
class QAState(TypedDict):
    styles: Annotated[str, 'Style question']
    number: Annotated[int, 'Number of output sentences per style']
    sentence: Annotated[str, 'User input sentence']
    generation: Annotated[str, 'LLM generated answer']
    messages: Annotated[list[ChatMessage], add_messages]

# ì¡°ê±´ ëª¨ë¸ ì •ì˜
class Condition(BaseModel):
    check: str = Field(description="Indicate 'yes' or 'no' whether it is OK")

    
# def init_state(state: QAState) -> QAState:
#     state['messages'] = [ChatMessage(role='system', content=system_prompt)]
#     state['styles'] = basic_styles
#     state['number'] = 1
#     return state

# ë…¸ë“œ(Node) ì •ì˜
def init(state: QAState) -> QAState:
    if not state['messages']:
        state['messages'] = [ChatMessage(role='system', content=system_prompt)]
    if not state['styles']:
        state['styles'] = basic_styles
        state['number'] = 1
    return state

def set_generation(state: QAState) -> QAState:
    add_kwargs = {'sentence': state['sentence']}        # ì›ë³¸ ì…ë ¥ ë¬¸ì¥
    content = "ì…ë ¥í•˜ì‹  ë¬¸ì¥ì´ ì´í•´í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì •í™•íˆ ì…ë ¥í•´ ì£¼ì„¸ìš”."
    messages = state['messages']
    messages.append(ChatMessage(role='user', content=content, additional_kwargs=add_kwargs))
    messages.append(ChatMessage(role='assistant', content=content))
    return QAState(messages=messages, generation=content)
    
def rectify(state: QAState) -> QAState:
    prompt = retify_prompt.format(**state)
    response = llm.invoke([system_prompt, prompt])
    return QAState(messages=state['messages'], generation=response.content, sentence=state['sentence'])

def generate(state: QAState) -> QAState:
    add_kwargs = {'sentence': state['sentence']}        # ì›ë³¸ ì…ë ¥ ë¬¸ì¥
    state['sentence'] = state['generation']     # ìˆ˜ì •ëœ ì…ë ¥ ë¬¸ì¥ì„ ì›ë³¸ ì…ë ¥ ë¬¸ì¥ì— ì„¤ì •
    prompt = generate_prompt.format(**state)
    messages = state['messages']
    messages.append(ChatMessage(role='user', content=prompt, additional_kwargs=add_kwargs))
    # chain = ChatPromptTemplate.from_messages(messages) | llm | StrOutputParser()
    # response = chain.invoke({})
    response = llm.invoke(messages)
    messages.append(ChatMessage(role='assistant', content=response.content))
    return QAState(messages=messages, generation=response.content)

def check_Sentence(state: QAState):
    structured_llm = llm.with_structured_output(Condition)
    temp_prompt = ChatPromptTemplate.from_messages([
        ('system', system_prompt),
        ('user', check_sentence_prompt)
    ])
    checker = temp_prompt | structured_llm
    response = checker.invoke(state)
    print(response.check)
    if response.check == 'yes':
        return 'rectify'
    else:
        return 'set_generation'



# ëª¨ë¸ êµ¬ì¶•
def compiled_graph():
    # ìƒíƒœ(State) ì •ì˜
    graph = StateGraph(QAState)
    # ë…¸ë“œ(Node) ì •ì˜
    graph.add_node('init', init)
    graph.add_node('set_generation', set_generation)
    graph.add_node('rectify', rectify)
    graph.add_node('generate', generate)
    # íë¦„(Edge) ì—°ê²°
    graph.set_entry_point('init')
    # graph.add_edge('init', 'rectify')
    graph.add_conditional_edges('init', check_Sentence)
    graph.add_edge('rectify', 'generate')
    graph.add_edge('set_generation', END)
    graph.add_edge('generate', END)
    # ê·¸ë˜í”„ ë¹Œë“œ(ì»´íŒŒì¼)
    return graph.compile()

# ì‹¤í–‰
def invoke(state, style=None):
    app = compiled_graph()
    result = app.invoke(state)
    return result



if __name__ == "__main__":
    state = {'sentence': 'asdfí•œsê¸€d'}
    result = invoke(state)
    print(result['generation'])
    
    state.update(result)
    state.update({'sentence': 'ì ì„ ëª»ì¤ë”ë‹ˆ ë¨¸ë¦¬ê°€ í„°ì§€ê² ë„¤'})
    result = invoke(state)
    print(result['generation'])
        
    state.update(result)
    state.update({'sentence': 'í•˜ë§ˆë™ê°€ë‘ë‰˜'})
    result = invoke(state)
    print(result['generation'])
        
    state.update(result)
    state.update({'sentence': 'ë˜ëŠ”ì¼ì´í•˜ë‚˜ë‘ì—„ë„¤'})
    result = invoke(state)
    for i, x in enumerate(result['messages']):
        print(i, x.role, x.additional_kwargs.get('sentence'), x.content[:10], '...')
